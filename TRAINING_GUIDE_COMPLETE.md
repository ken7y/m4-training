# Complete Training Guide - M4 + Reddit Balanced Dataset

**Quick Start for RunPod Training**

---

## Dataset Overview

### What We Have
- **60k Reddit posts** (casual human text, pre-ChatGPT, already cleaned)
- **63k M4 samples** (formal text: academic papers, Wikipedia - now cleaned)

### Class Distribution (English Only)
```
M4 ENGLISH cleaned: 63,117 lines (only arxiv, wikipedia, wikihow, peerread, reddit)
  Each line = human_text (label=0) + machine_text (label=1)
  â†’ 63,117 human + 63,117 AI from M4

Reddit 60k: 60,000 samples (all label=0, human, casual text)

TOTAL:
  Human: 123,117 (66.1%) - 60k casual + 63k formal
  AI:     63,117 (33.9%) - all formal
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL: 186,234 samples
```

**Note**: Only ~2:1 imbalance (66% human, 34% AI) - this is MILD and manageable with weighted loss

---

## Solutions for Class Imbalance

### Option 1: Weighted Loss (RECOMMENDED)
Use weighted cross-entropy where AI class gets higher weight.

**Advantages:**
âœ… Uses all data (no waste)  
âœ… Easy to implement  
âœ… Works well with RoBERTa  
âœ… Standard practice  
âœ… Already built into `train.py` when you pass `--use_class_weights`

**Implementation:**
```python
# In training script
from torch.nn import CrossEntropyLoss

# Calculate class weights (English only)
# M4 English: 63,117 human + 63,117 AI
# Reddit: 60,000 human
num_ai = 63117
total = num_human + num_ai

weight_human = total / (2 * num_human)  # 0.756
weight_ai = total / (2 * num_ai)        # 1.476
    --stratified_split \
    --validation_split 0.1 \

    --use_class_weights \
class_weights = torch.tensor([weight_human, weight_ai])
# AI loss weighted 1.95x higher than human

# Use in Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    # Add weighted loss
    compute_loss=lambda model, inputs, return_outputs=False: 
        weighted_loss(model, inputs, class_weights, return_outputs)
)
```

### Option 2: Undersample Human Text
Balance by randomly removing human samples.

**Result:**
```
Human: 63,117 (50%)
AI:    63,117 (50%)
Total: 126,234 samples
```

**Advantages:**
âœ… Perfect balance  
âœ… Simpler training  

**Disadvantages:**
âŒ Wastes 60k human samples  
âŒ Less diversity  

### Option 3: Oversample AI Text
Duplicate AI samples to match human count.

**Disadvantages:**
âŒ Overfitting risk  
âŒ Not recommended for text  

---

## Quick Start Commands

### 1. Clean M4 Data (if not done)
```bash
python3 clean_m4_data.py
```

### 2. Train with Weighted Loss & Stratified Split
```bash
python3 train.py \
  --data_dir M4_cleaned/data \
  --train_domains arxiv wikipedia reddit wikihow peerread \
    --train_generators chatGPT davinci cohere dolly flant5 \
  --val_generator flant5 \
  --epochs 3 \
  --batch_size 64 \
  --bf16 \
  --use_wandb \
  --run_name balanced-weighted \
  --output_dir runs/balanced-weighted
```

---

## Consolidated Documentation

All training documentation is now in this file:
- âœ… M4 cleaning (completed: 63k samples, 92% retention)
- âœ… Class imbalance handling (weighted loss recommended)
- âœ… Stratified splitting (preserves 66/34 ratio)
- âœ… Complete training script

**Other MD files to keep:**
- `README.md` - Project overview
- `QUICKSTART.md` - Quick commands
- `TESTING_GUIDE.md` - Model evaluation

**MD files to archive/delete:**
- `M4_CLEANING_GUIDE.md` â†’ consolidated here
- `M4_CLEANING_QUICK_REF.md` â†’ consolidated here
- `RUNPOD_TRAINING.md` â†’ consolidated here

---

## Expected Results

### Training Time (A6000)
- ~6-8 hours for 3 epochs with 186k samples

### Performance Expectations
- **Accuracy**: 93-96%
- **F1 Score**: 92-95%
- **False Positive Rate**: <5% on casual human text

### Benefits of This Approach
âœ… Uses all data (no waste)  
âœ… Weighted loss handles imbalance  
âœ… Stratified split preserves ratio  
âœ… Cleaned data reduces bias  
âœ… Balanced formal + casual human text  

---

## Summary

**Dataset:** 186k samples (66% human, 34% AI)  
**Solution:** Weighted loss + stratified split  
**Training:** ~6-8 hours on A6000  
**Output:** Balanced model with low false positives  

ğŸš€ Ready to train on RunPod!
