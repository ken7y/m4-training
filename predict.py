#!/usr/bin/env python3
"""
M4 Inference Script with Chunking
Averages predictions across chunks for final decision (per paper methodology)
"""

import argparse
import json
import os
import re
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax
from typing import List, Dict
from tqdm import tqdm


def parse_args():
    parser = argparse.ArgumentParser(description="Run inference on M4 detector")
    parser.add_argument("--model_path", type=str, required=True, help="Path to trained model")
    parser.add_argument("--input_file", type=str, required=True, help="Input JSONL file")
    parser.add_argument("--output_file", type=str, default="predictions.jsonl", help="Output file")
    parser.add_argument("--max_length", type=int, default=512, help="Max sequence length")
    parser.add_argument("--chunk_overlap", type=int, default=50, help="Token overlap between chunks")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for inference")
    parser.add_argument("--normalize_digits", action="store_true", default=True, help="Replace digits with '1'")
    return parser.parse_args()


def normalize_digits(text: str) -> str:
    """Replace all digits with '1' while keeping format"""
    return re.sub(r'\d', '1', text)


def chunk_text_with_overlap(text: str, tokenizer, max_length: int = 512, overlap: int = 50) -> List[str]:
    """
    Chunk text into overlapping segments that fit within max_length tokens.
    """
    tokens = tokenizer.encode(text, add_special_tokens=False, truncation=False)
    max_tokens_per_chunk = max_length - 2  # Account for [CLS] and [SEP]

    if len(tokens) <= max_tokens_per_chunk:
        return [text]

    chunks = []
    start_idx = 0

    while start_idx < len(tokens):
        end_idx = min(start_idx + max_tokens_per_chunk, len(tokens))
        chunk_tokens = tokens[start_idx:end_idx]
        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
        chunks.append(chunk_text)

        if end_idx >= len(tokens):
            break
        start_idx += max_tokens_per_chunk - overlap

    return chunks


def predict_text(text: str, model, tokenizer, device, max_length=512, chunk_overlap=50, normalize=True) -> Dict:
    """
    Predict if text is AI-generated by:
    1. Chunking text
    2. Getting predictions for each chunk
    3. Averaging posterior probabilities
    4. Making final decision
    """
    # Preprocess
    if normalize:
        text = normalize_digits(text)

    # Chunk text
    chunks = chunk_text_with_overlap(text, tokenizer, max_length, chunk_overlap)

    # Get predictions for all chunks
    all_probs = []

    for chunk in chunks:
        inputs = tokenizer(
            chunk,
            return_tensors='pt',
            padding='max_length',
            truncation=True,
            max_length=max_length
        ).to(device)

        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits.cpu().numpy()[0]
            probs = softmax(logits)
            all_probs.append(probs)

    # Average probabilities across chunks
    avg_probs = np.mean(all_probs, axis=0)

    # Make final prediction
    prediction = int(np.argmax(avg_probs))
    confidence = float(avg_probs[prediction])

    return {
        'prediction': prediction,  # 0 = human, 1 = machine
        'confidence': confidence,
        'prob_human': float(avg_probs[0]),
        'prob_machine': float(avg_probs[1]),
        'num_chunks': len(chunks),
    }


def main():
    args = parse_args()

    # Load model and tokenizer
    print(f"Loading model from {args.model_path}")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    model = AutoModelForSequenceClassification.from_pretrained(args.model_path)
    model.to(device)
    model.eval()

    # Load training config if available
    config_path = os.path.join(args.model_path, "training_config.json")
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            train_config = json.load(f)
            print(f"Loaded training config: {train_config.get('model', 'unknown')}")
            print(f"  Trained on generators: {train_config.get('train_generators', [])}")
            print(f"  Val generator: {train_config.get('val_generator', 'unknown')}")

    # Load input data
    print(f"\nLoading data from {args.input_file}")
    with open(args.input_file, 'r', encoding='utf-8') as f:
        data = [json.loads(line.strip()) for line in f if line.strip()]

    print(f"Loaded {len(data)} samples")

    # Run predictions
    print(f"\nRunning predictions...")
    results = []

    for item in tqdm(data):
        # M4 format: check for both machine_text/human_text or generic text field
        text = item.get('text', item.get('content', ''))

        # If text not found, check if this is M4 format with both human_text and machine_text
        if not text:
            # For M4 inference, we typically want to predict on machine_text
            # But you can change this to 'human_text' if needed
            text = item.get('machine_text', item.get('human_text', ''))

        if not text:
            continue

        pred_result = predict_text(
            text,
            model,
            tokenizer,
            device,
            max_length=args.max_length,
            chunk_overlap=args.chunk_overlap,
            normalize=args.normalize_digits
        )

        result = {
            'id': item.get('id', len(results)),
            'prediction': pred_result['prediction'],
            'label': 'machine' if pred_result['prediction'] == 1 else 'human',
            'confidence': pred_result['confidence'],
            'prob_human': pred_result['prob_human'],
            'prob_machine': pred_result['prob_machine'],
            'num_chunks': pred_result['num_chunks'],
        }

        # Include ground truth if available
        if 'label' in item:
            result['true_label'] = item['label']

        results.append(result)

    # Save results
    print(f"\nSaving predictions to {args.output_file}")
    with open(args.output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result) + '\n')

    # Print summary statistics
    print(f"\nðŸ“Š Summary:")
    print(f"  Total samples: {len(results)}")
    print(f"  Predicted human: {sum(1 for r in results if r['prediction'] == 0)}")
    print(f"  Predicted machine: {sum(1 for r in results if r['prediction'] == 1)}")
    print(f"  Avg confidence: {np.mean([r['confidence'] for r in results]):.3f}")
    print(f"  Avg chunks per text: {np.mean([r['num_chunks'] for r in results]):.1f}")

    # If ground truth available, compute accuracy
    if all('true_label' in r for r in results):
        correct = sum(1 for r in results if r['prediction'] == r['true_label'])
        accuracy = correct / len(results)
        print(f"  Accuracy: {accuracy:.3f} ({correct}/{len(results)})")

    print(f"\nâœ… Done! Results saved to {args.output_file}")


if __name__ == "__main__":
    main()
